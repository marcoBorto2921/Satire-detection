{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhv5_qtP_n7K"
      },
      "source": [
        "Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DixW7-62_3ny"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import json\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, cohen_kappa_score, confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0TBhwAu_1G6",
        "outputId": "c9ebe3fa-06fb-442a-fce0-5fcfb1552361"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKR6qWGrABYK"
      },
      "source": [
        "# cross validation's final results of"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltwdsmFc_j5u",
        "outputId": "21670a45-0ea3-4420-b116-06c6c5cb087a"
      },
      "outputs": [],
      "source": [
        "# Folder where the results are saved\n",
        "base_save_path = 'Insert path to saved results'\n",
        "num_splits = 10\n",
        "\n",
        "# Dictionary to accumulate metrics\n",
        "metrics = {\n",
        "    \"accuracy\": [],\n",
        "    \"f1\": [],\n",
        "    \"precision\": [],\n",
        "    \"recall\": [],\n",
        "    \"cohen_kappa\": []\n",
        "}\n",
        "\n",
        "# Loop over all splits\n",
        "for i in range(1, num_splits + 1):\n",
        "    results_file = os.path.join(base_save_path, f'Cross {i}', 'test_results.json')\n",
        "\n",
        "    if os.path.exists(results_file):\n",
        "        with open(results_file, 'r') as f:\n",
        "            res = json.load(f)\n",
        "\n",
        "        # Append main metrics\n",
        "        for k in metrics.keys():\n",
        "            if k in res:\n",
        "                metrics[k].append(res[k])\n",
        "    else:\n",
        "        print(f\" Missing results for Cross {i}\")\n",
        "\n",
        "# Compute mean and standard deviation\n",
        "final_results = {}\n",
        "for k, values in metrics.items():\n",
        "    if len(values) > 0:\n",
        "        final_results[k] = {\n",
        "            \"mean\": np.mean(values),\n",
        "            \"std\": np.std(values)\n",
        "        }\n",
        "\n",
        "# Print summary\n",
        "print(\"\\n=== FINAL CROSS-VALIDATION RESULTS ===\")\n",
        "for k, v in final_results.items():\n",
        "    print(f\"{k}: {v['mean']:.4f} ± {v['std']:.4f}\")\n",
        "\n",
        "# Save summary as JSON\n",
        "summary_file = os.path.join(base_save_path, 'cross_validation_summary.json')\n",
        "with open(summary_file, 'w') as f:\n",
        "    json.dump(final_results, f, indent=4)\n",
        "\n",
        "print(f\"\\nSummary saved at: {summary_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67QLvE9BZwRf"
      },
      "source": [
        "# Results without oultiers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NkDJXYDBZnkp",
        "outputId": "340b50b8-5cdf-43e4-d130-37f945137bbe"
      },
      "outputs": [],
      "source": [
        "# Folder where the results are saved\n",
        "base_save_path = 'Insert path to saved results'\n",
        "num_splits = 10\n",
        "\n",
        "# Dictionary to accumulate metrics\n",
        "metrics = {\n",
        "    \"accuracy\": [],\n",
        "    \"f1\": [],\n",
        "    \"precision\": [],\n",
        "    \"recall\": [],\n",
        "    \"cohen_kappa\": []\n",
        "}\n",
        "\n",
        "# Loop over all splits\n",
        "for i in range(1, num_splits + 1):\n",
        "    results_file = os.path.join(base_save_path, f'Cross {i}', 'test_results.json')\n",
        "\n",
        "    if os.path.exists(results_file):\n",
        "        with open(results_file, 'r') as f:\n",
        "            res = json.load(f)\n",
        "\n",
        "        # Append main metrics\n",
        "        for k in metrics.keys():\n",
        "            if k in res:\n",
        "                metrics[k].append(res[k])\n",
        "    else:\n",
        "        print(f\" Missing results for Cross {i}\")\n",
        "\n",
        "# Identify and remove folds with best and worst F1 scores\n",
        "f1_scores = np.array(metrics[\"f1\"])\n",
        "best_idx = np.argmax(f1_scores)\n",
        "worst_idx = np.argmin(f1_scores)\n",
        "\n",
        "print(f\"\\nRemoving outliers:\")\n",
        "print(f\" - Best fold (F1={f1_scores[best_idx]:.4f}) → removed\")\n",
        "print(f\" - Worst fold (F1={f1_scores[worst_idx]:.4f}) → removed\")\n",
        "\n",
        "# Remove those indices from all metrics\n",
        "indices_to_keep = [i for i in range(len(f1_scores)) if i not in [best_idx, worst_idx]]\n",
        "metrics_filtered = {k: [v[i] for i in indices_to_keep] for k, v in metrics.items()}\n",
        "\n",
        "# Compute mean and standard deviation after removal\n",
        "final_results_filtered = {}\n",
        "for k, values in metrics_filtered.items():\n",
        "    if len(values) > 0:\n",
        "        final_results_filtered[k] = {\n",
        "            \"mean\": np.mean(values),\n",
        "            \"std\": np.std(values)\n",
        "        }\n",
        "\n",
        "# Print final summary\n",
        "print(\"\\n=== FINAL CROSS-VALIDATION RESULTS (without outliers) ===\")\n",
        "for k, v in final_results_filtered.items():\n",
        "    print(f\"{k}: {v['mean']:.4f} ± {v['std']:.4f}\")\n",
        "\n",
        "# Save summary as JSON\n",
        "summary_file_filtered = os.path.join(base_save_path, 'cross_validation_summary_no_outliers.json')\n",
        "with open(summary_file_filtered, 'w') as f:\n",
        "    json.dump(final_results_filtered, f, indent=4)\n",
        "\n",
        "print(f\"\\nSummary (without outliers) saved at: {summary_file_filtered}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvV-cwDgAaIG"
      },
      "source": [
        "# Global analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vEGyPNIGAbhF",
        "outputId": "23822afb-8ba3-4158-ac61-c6c8ab29ccd8"
      },
      "outputs": [],
      "source": [
        "# Aggregated evaluation over all predictions\n",
        "all_true = []\n",
        "all_pred = []\n",
        "\n",
        "for i in range(1, num_splits + 1):\n",
        "    folder = os.path.join(base_save_path, f'Cross {i}')\n",
        "    true_path = os.path.join(folder, 'true_labels.npy')\n",
        "    pred_path = os.path.join(folder, 'predicted_labels.npy')\n",
        "\n",
        "    if os.path.exists(true_path) and os.path.exists(pred_path):\n",
        "        true_labels = np.load(true_path)\n",
        "        pred_labels = np.load(pred_path)\n",
        "        all_true.extend(true_labels)\n",
        "        all_pred.extend(pred_labels)\n",
        "    else:\n",
        "        print(f\" Missing predictions for Cross {i}\")\n",
        "\n",
        "if len(all_true) > 0:\n",
        "    all_true = np.array(all_true)\n",
        "    all_pred = np.array(all_pred)\n",
        "\n",
        "    # Compute global metrics\n",
        "    acc_global = accuracy_score(all_true, all_pred)\n",
        "    f1_global = f1_score(all_true, all_pred, average=\"weighted\")\n",
        "    prec_global = precision_score(all_true, all_pred, average=\"weighted\")\n",
        "    rec_global = recall_score(all_true, all_pred, average=\"weighted\")\n",
        "    kappa_global = cohen_kappa_score(all_true, all_pred)\n",
        "    cm_global = confusion_matrix(all_true, all_pred)\n",
        "\n",
        "    print(\"\\n=== GLOBAL EVALUATION ACROSS ALL SPLITS ===\")\n",
        "    print(f\"Accuracy: {acc_global:.4f}\")\n",
        "    print(f\"F1-score: {f1_global:.4f}\")\n",
        "    print(f\"Precision: {prec_global:.4f}\")\n",
        "    print(f\"Recall: {rec_global:.4f}\")\n",
        "    print(f\"Cohen’s Kappa: {kappa_global:.4f}\")\n",
        "    print(\"Confusion Matrix:\\n\", cm_global)\n",
        "\n",
        "    # Save global results as JSON\n",
        "    global_results = {\n",
        "        \"accuracy\": acc_global,\n",
        "        \"f1\": f1_global,\n",
        "        \"precision\": prec_global,\n",
        "        \"recall\": rec_global,\n",
        "        \"cohen_kappa\": kappa_global,\n",
        "        \"confusion_matrix\": cm_global.tolist()\n",
        "    }\n",
        "\n",
        "    global_file = os.path.join(base_save_path, 'cross_validation_global_results.json')\n",
        "    with open(global_file, 'w') as f:\n",
        "        json.dump(global_results, f, indent=4)\n",
        "\n",
        "    print(f\"\\nGlobal results saved at: {global_file}\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
