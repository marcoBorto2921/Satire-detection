{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQ9gcj3-9HNh"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o6PME8e89I8N"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, cohen_kappa_score, confusion_matrix, classification_report\n",
        "from sklearn.metrics import precision_score, recall_score, cohen_kappa_score\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "btpC85Q89NvG",
        "outputId": "ac33376d-0b08-4b46-88e8-dc16bb209e08"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFRpWvhVpVkA"
      },
      "source": [
        "# Evaluation Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fMOr6--P8oJf"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(true_labels, pred_labels, class_names=None, title=\"Confusion Matrix\"):\n",
        "    \"\"\"\n",
        "    Computes and displays classification metrics and the confusion matrix.\n",
        "\n",
        "    Args:\n",
        "        true_labels (list or np.array): Ground-truth labels.\n",
        "        pred_labels (list or np.array): Predicted labels.\n",
        "        class_names (list, optional): Class names (e.g. ['no_satire', 'satire']).\n",
        "        title (str): Title of the confusion matrix plot.\n",
        "    \"\"\"\n",
        "\n",
        "    if class_names is None:\n",
        "        class_names = sorted(list(set(true_labels)))\n",
        "\n",
        "    # Compute global evaluation metrics\n",
        "    acc = accuracy_score(true_labels, pred_labels)\n",
        "    prec = precision_score(true_labels, pred_labels, average='weighted', zero_division=0)\n",
        "    rec = recall_score(true_labels, pred_labels, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(true_labels, pred_labels, average='weighted', zero_division=0)\n",
        "    kappa = cohen_kappa_score(true_labels, pred_labels)\n",
        "\n",
        "    # Print evaluation metrics\n",
        "    print(f\"Accuracy: {acc:.4f}\")\n",
        "    print(f\"Precision (weighted): {prec:.4f}\")\n",
        "    print(f\"Recall (weighted): {rec:.4f}\")\n",
        "    print(f\"F1-score (weighted): {f1:.4f}\")\n",
        "    print(f\"Cohen’s Kappa: {kappa:.4f}\")\n",
        "\n",
        "    # Print detailed per-class report\n",
        "    print(\"\\nClassification report:\")\n",
        "    print(\n",
        "        classification_report(\n",
        "            true_labels,\n",
        "            pred_labels,\n",
        "            target_names=class_names,\n",
        "            zero_division=0\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    cm = confusion_matrix(true_labels, pred_labels)\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    sns.heatmap(\n",
        "        cm,\n",
        "        annot=True,\n",
        "        fmt=\"d\",\n",
        "        cmap=\"Blues\",\n",
        "        xticklabels=class_names,\n",
        "        yticklabels=class_names\n",
        "    )\n",
        "    plt.xlabel(\"Predicted Label\")\n",
        "    plt.ylabel(\"True Label\")\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "    # Return computed metrics\n",
        "    return {\n",
        "        \"accuracy\": acc,\n",
        "        \"precision\": prec,\n",
        "        \"recall\": rec,\n",
        "        \"f1\": f1,\n",
        "        \"cohen_kappa\": kappa\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iz1XMvJ4_AaO"
      },
      "source": [
        "# Save results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93o2JUG0_B7s"
      },
      "outputs": [],
      "source": [
        "def save_test_results(true_labels, pred_labels, class_names, save_path):\n",
        "    \"\"\"\n",
        "    Computes and saves evaluation metrics and the confusion matrix to disk.\n",
        "\n",
        "    Args:\n",
        "        true_labels (list or np.array): Ground-truth labels.\n",
        "        pred_labels (list or np.array): Predicted labels.\n",
        "        class_names (list): List of class names.\n",
        "        save_path (str): Directory where results will be saved.\n",
        "    \"\"\"\n",
        "\n",
        "    # Compute base metrics using the evaluation function\n",
        "    metrics = evaluate_model(\n",
        "        true_labels,\n",
        "        pred_labels,\n",
        "        class_names,\n",
        "        title=\"Confusion Matrix\"\n",
        "    )\n",
        "\n",
        "    # Add additional metrics\n",
        "    metrics[\"precision_macro\"] = precision_score(\n",
        "        true_labels,\n",
        "        pred_labels,\n",
        "        average=\"macro\",\n",
        "        zero_division=0\n",
        "    )\n",
        "    metrics[\"recall_macro\"] = recall_score(\n",
        "        true_labels,\n",
        "        pred_labels,\n",
        "        average=\"macro\",\n",
        "        zero_division=0\n",
        "    )\n",
        "    metrics[\"cohen_kappa\"] = cohen_kappa_score(true_labels, pred_labels)\n",
        "\n",
        "    # Save metrics to JSON file\n",
        "    metrics_path = os.path.join(save_path, \"metrics.json\")\n",
        "    with open(metrics_path, \"w\") as f:\n",
        "        json.dump(metrics, f, indent=4)\n",
        "\n",
        "    print(f\"Metrics saved to: {metrics_path}\")\n",
        "\n",
        "    # Generate and save the confusion matrix plot\n",
        "    cm = confusion_matrix(true_labels, pred_labels)\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    sns.heatmap(\n",
        "        cm,\n",
        "        annot=True,\n",
        "        fmt=\"d\",\n",
        "        cmap=\"Blues\",\n",
        "        xticklabels=class_names,\n",
        "        yticklabels=class_names\n",
        "    )\n",
        "    plt.xlabel(\"Predicted Label\")\n",
        "    plt.ylabel(\"True Label\")\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "\n",
        "    cm_path = os.path.join(save_path, \"confusion_matrix.png\")\n",
        "    plt.savefig(cm_path, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    print(f\"Confusion matrix saved to: {cm_path}\")\n",
        "\n",
        "    return metrics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scqOi6Kn8Vg9"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6B8MhRL78WUX",
        "outputId": "4f275035-94b6-4b84-ec68-e180770631c2"
      },
      "outputs": [],
      "source": [
        "# General parameters\n",
        "base_save_path = \"Insert multimodal results saving path\"\n",
        "base_path = \"Insert cross-validation splits path\"\n",
        "\n",
        "# NumPy files paths\n",
        "audio_numpy_path = \"Insert audio model NumPy outputs path\"\n",
        "text_numpy_path = \"Insert text model NumPy outputs path\"\n",
        "\n",
        "num_splits = 10\n",
        "class_names = ['no_satire', 'satire']\n",
        "\n",
        "# Weights to test (must sum to 1)\n",
        "weights = [0.0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5]\n",
        "direct_pairs = [(w, 1 - w) for w in weights]\n",
        "inverse_pairs = [(1 - w, w) for w in weights]\n",
        "\n",
        "# Merge and remove duplicate pairs (e.g. (0.5, 0.5))\n",
        "weight_pairs = list({pair for pair in direct_pairs + inverse_pairs})\n",
        "weight_pairs = sorted(weight_pairs, key=lambda x: x[0])\n",
        "\n",
        "# Loop over all cross-validation splits\n",
        "for i in range(1, num_splits + 1):\n",
        "    print(f\"\\nProcessing Cross {i}\")\n",
        "\n",
        "    # Paths to NumPy files\n",
        "    split_path_audio = os.path.join(audio_numpy_path, f'Cross {i}', 'probabilities.npy')\n",
        "    split_path_text = os.path.join(text_numpy_path, f'Cross {i}', 'probabilities.npy')\n",
        "    true_labels_path = os.path.join(text_numpy_path, f'Cross {i}', 'true_labels.npy')\n",
        "\n",
        "    # Load NumPy arrays\n",
        "    audio_probabilities = np.load(split_path_audio)\n",
        "    text_probabilities = np.load(split_path_text)\n",
        "    true_labels = np.load(true_labels_path)\n",
        "\n",
        "    if not isinstance(text_probabilities, np.ndarray):\n",
        "        text_probabilities = np.array([t.cpu().numpy() for t in text_probabilities])\n",
        "\n",
        "    # Evaluate single-modality models\n",
        "    audio_pred_classes = np.argmax(audio_probabilities, axis=1)\n",
        "    text_pred_classes = np.argmax(text_probabilities, axis=1)\n",
        "\n",
        "    metrics_audio = {\n",
        "        \"accuracy\": accuracy_score(true_labels, audio_pred_classes),\n",
        "        \"f1\": f1_score(true_labels, audio_pred_classes, average='weighted'),\n",
        "        \"precision\": precision_score(true_labels, audio_pred_classes, average='weighted'),\n",
        "        \"recall\": recall_score(true_labels, audio_pred_classes, average='weighted'),\n",
        "        \"kappa\": cohen_kappa_score(true_labels, audio_pred_classes)\n",
        "    }\n",
        "\n",
        "    metrics_text = {\n",
        "        \"accuracy\": accuracy_score(true_labels, text_pred_classes),\n",
        "        \"f1\": f1_score(true_labels, text_pred_classes, average='weighted'),\n",
        "        \"precision\": precision_score(true_labels, text_pred_classes, average='weighted'),\n",
        "        \"recall\": recall_score(true_labels, text_pred_classes, average='weighted'),\n",
        "        \"kappa\": cohen_kappa_score(true_labels, text_pred_classes)\n",
        "    }\n",
        "\n",
        "    print(f\"Audio model | Acc: {metrics_audio['accuracy']:.4f}, F1: {metrics_audio['f1']:.4f}\")\n",
        "    print(f\"Text model  | Acc: {metrics_text['accuracy']:.4f}, F1: {metrics_text['f1']:.4f}\")\n",
        "\n",
        "    results = []\n",
        "\n",
        "    # Test all weight combinations\n",
        "    for alpha_audio, alpha_text in weight_pairs:\n",
        "        final_pred = alpha_audio * audio_probabilities + alpha_text * text_probabilities\n",
        "        final_pred_classes = np.argmax(final_pred, axis=1)\n",
        "\n",
        "        acc = accuracy_score(true_labels, final_pred_classes)\n",
        "        f1 = f1_score(true_labels, final_pred_classes, average='weighted')\n",
        "        precision = precision_score(true_labels, final_pred_classes, average='weighted')\n",
        "        recall = recall_score(true_labels, final_pred_classes, average='weighted')\n",
        "        kappa = cohen_kappa_score(true_labels, final_pred_classes)\n",
        "\n",
        "        results.append({\n",
        "            'alpha_audio': alpha_audio,\n",
        "            'alpha_text': alpha_text,\n",
        "            'accuracy': acc,\n",
        "            'f1': f1,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'kappa': kappa\n",
        "        })\n",
        "\n",
        "    # Create results DataFrame\n",
        "    df_results = pd.DataFrame(results)\n",
        "    df_results = df_results.sort_values(by='f1', ascending=False).reset_index(drop=True)\n",
        "\n",
        "    # Best-performing combination\n",
        "    best_result = df_results.iloc[0]\n",
        "    best_audio = best_result['alpha_audio']\n",
        "    best_text = best_result['alpha_text']\n",
        "\n",
        "    # Specific combination (audio 0.45, text 0.55)\n",
        "    specific_result = df_results[\n",
        "        (df_results['alpha_audio'] == 0.45) & (df_results['alpha_text'] == 0.55)\n",
        "    ]\n",
        "\n",
        "    if specific_result.empty:\n",
        "        specific_result = pd.DataFrame([{\n",
        "            'alpha_audio': 0.45,\n",
        "            'alpha_text': 0.55,\n",
        "            'accuracy': np.nan,\n",
        "            'f1': np.nan,\n",
        "            'precision': np.nan,\n",
        "            'recall': np.nan,\n",
        "            'kappa': np.nan\n",
        "        }])\n",
        "\n",
        "    # Save results\n",
        "    save_path = os.path.join(base_save_path, f'Cross {i}')\n",
        "    os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "    df_results.to_csv(os.path.join(save_path, 'all_results.csv'), index=False)\n",
        "    best_result.to_frame().T.to_csv(os.path.join(save_path, 'best_result.csv'), index=False)\n",
        "    specific_result.to_csv(\n",
        "        os.path.join(save_path, 'specific_result_0.45_0.55.csv'),\n",
        "        index=False\n",
        "    )\n",
        "\n",
        "    print(f\"Results saved to: {save_path}\")\n",
        "    print(\n",
        "        f\"Best fusion → Audio {best_audio:.2f}, Text {best_text:.2f} | \"\n",
        "        f\"F1: {best_result['f1']:.4f}\"\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NslJ_wV3PPQ"
      },
      "source": [
        "# Best results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gx7Xx0jN3Ods",
        "outputId": "d4fac716-e92d-4988-aab0-cdfe70a20917"
      },
      "outputs": [],
      "source": [
        "# Base path\n",
        "base_save_path = \"Insert multimodal results base path\"\n",
        "num_splits = 10\n",
        "\n",
        "# List to store DataFrames from all folds\n",
        "all_results = []\n",
        "\n",
        "print(\"\\nLoading results from all cross-validation folds\")\n",
        "for i in range(1, num_splits + 1):\n",
        "    path = os.path.join(base_save_path, f'Cross {i}', 'all_results.csv')\n",
        "    if os.path.exists(path):\n",
        "        df = pd.read_csv(path)\n",
        "        df['fold'] = i\n",
        "        all_results.append(df)\n",
        "        print(f\"Loaded Cross {i} ({len(df)} rows)\")\n",
        "    else:\n",
        "        print(f\"Missing file for Cross {i}: {path}\")\n",
        "\n",
        "# Merge all results\n",
        "if len(all_results) == 0:\n",
        "    raise ValueError(\"No result files found. Please check the result paths.\")\n",
        "\n",
        "df_all = pd.concat(all_results, ignore_index=True)\n",
        "\n",
        "# Group by weight combinations and compute mean and std\n",
        "summary = (\n",
        "    df_all.groupby(['alpha_audio', 'alpha_text'])\n",
        "    .agg({\n",
        "        'accuracy': ['mean', 'std'],\n",
        "        'f1': ['mean', 'std'],\n",
        "        'precision': ['mean', 'std'],\n",
        "        'recall': ['mean', 'std'],\n",
        "        'kappa': ['mean', 'std']\n",
        "    })\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "# Rename columns\n",
        "summary.columns = [\n",
        "    'alpha_audio', 'alpha_text',\n",
        "    'accuracy_mean', 'accuracy_std',\n",
        "    'f1_mean', 'f1_std',\n",
        "    'precision_mean', 'precision_std',\n",
        "    'recall_mean', 'recall_std',\n",
        "    'kappa_mean', 'kappa_std'\n",
        "]\n",
        "\n",
        "# Sort by mean F1-score\n",
        "summary = summary.sort_values(by='f1_mean', ascending=False).reset_index(drop=True)\n",
        "\n",
        "# Print top 5 combinations\n",
        "print(\"\\nTop 5 weight combinations (averaged across all folds)\")\n",
        "print(summary.head(5))\n",
        "\n",
        "# Save final summary\n",
        "summary_path = os.path.join(base_save_path, 'summary_results.csv')\n",
        "summary.to_csv(summary_path, index=False)\n",
        "\n",
        "print(f\"\\nFinal summary file saved at:\\n{summary_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PD8XpbS4Y0j"
      },
      "source": [
        "# No outliers best Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iPOyN4pS4cRo",
        "outputId": "4bce6c86-d13b-4401-eae5-f8a38bb4a3b1"
      },
      "outputs": [],
      "source": [
        "# Base path for saving results\n",
        "base_save_path = \"Insert multimodal results base path\"\n",
        "num_splits = 10\n",
        "\n",
        "# List to store DataFrames from all folds\n",
        "all_results = []\n",
        "\n",
        "print(\"\\nLoading results from all cross-validation folds\")\n",
        "for i in range(1, num_splits + 1):\n",
        "    path = os.path.join(base_save_path, f'Cross {i}', 'all_results.csv')\n",
        "    if os.path.exists(path):\n",
        "        df = pd.read_csv(path)\n",
        "        df['fold'] = i\n",
        "        all_results.append(df)\n",
        "        print(f\"Loaded Cross {i} ({len(df)} rows)\")\n",
        "    else:\n",
        "        print(f\"Missing file for Cross {i}: {path}\")\n",
        "\n",
        "# Merge all results\n",
        "if len(all_results) == 0:\n",
        "    raise ValueError(\"No result files found. Please check the result paths.\")\n",
        "\n",
        "df_all = pd.concat(all_results, ignore_index=True)\n",
        "\n",
        "# Function to remove best and worst for a metric (default: F1)\n",
        "# Computes mean and std for all 5 metrics\n",
        "def remove_best_worst(group, metric='f1'):\n",
        "    if len(group) > 2:\n",
        "        group = group.sort_values(by=metric)\n",
        "        group = group.iloc[1:-1]  # remove worst and best\n",
        "\n",
        "    return pd.Series({\n",
        "        'accuracy_mean': group['accuracy'].mean(),\n",
        "        'accuracy_std': group['accuracy'].std(),\n",
        "\n",
        "        'f1_mean': group['f1'].mean(),\n",
        "        'f1_std': group['f1'].std(),\n",
        "\n",
        "        'precision_mean': group['precision'].mean(),\n",
        "        'precision_std': group['precision'].std(),\n",
        "\n",
        "        'recall_mean': group['recall'].mean(),\n",
        "        'recall_std': group['recall'].std(),\n",
        "\n",
        "        'kappa_mean': group['kappa'].mean(),\n",
        "        'kappa_std': group['kappa'].std()\n",
        "    })\n",
        "\n",
        "# Apply function for each weight combination\n",
        "summary = (\n",
        "    df_all.groupby(['alpha_audio', 'alpha_text'], group_keys=False)\n",
        "    .apply(remove_best_worst)\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "# Sort by mean F1-score\n",
        "summary = summary.sort_values(by='f1_mean', ascending=False).reset_index(drop=True)\n",
        "\n",
        "# Print top 5 combinations\n",
        "print(\"\\nTop 5 weight combinations (average across all folds, excluding best & worst)\")\n",
        "print(summary.head(5))\n",
        "\n",
        "# Save final filtered summary\n",
        "summary_path = os.path.join(base_save_path, 'summary_results_filtered.csv')\n",
        "summary.to_csv(summary_path, index=False)\n",
        "\n",
        "print(f\"\\nFinal filtered summary file saved at:\\n{summary_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4FwzVIxuAq-"
      },
      "source": [
        "# Cross-validation evaluation with external dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DGJqenV4uB3B",
        "outputId": "c89b9bb9-0947-4392-9624-265f01458978"
      },
      "outputs": [],
      "source": [
        "# General parameters \n",
        "base_save_path = 'Insert path to save external results'\n",
        "\n",
        "# NumPy files paths \n",
        "audio_numpy_path = 'Insert path to audio NumPy probabilities'\n",
        "text_numpy_path = 'Insert path to text NumPy probabilities'\n",
        "\n",
        "class_names = ['no_satire', 'satire']\n",
        "\n",
        "# Weights to try (sum must be 1) \n",
        "weights = [0.0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5]\n",
        "direct_pairs = [(w, 1 - w) for w in weights]\n",
        "inverse_pairs = [(1 - w, w) for w in weights]\n",
        "\n",
        "# Merge and remove duplicates like (0.5, 0.5)\n",
        "weight_pairs = list({pair for pair in direct_pairs + inverse_pairs})\n",
        "weight_pairs = sorted(weight_pairs, key=lambda x: x[0])\n",
        "\n",
        "print(f\"\\n=== Processing external dataset ===\")\n",
        "\n",
        "# File paths\n",
        "split_path_audio = os.path.join(audio_numpy_path, 'probabilities.npy')\n",
        "split_path_text = os.path.join(text_numpy_path, 'probabilities.npy')\n",
        "true_labels_path = os.path.join(text_numpy_path, 'true_labels.npy')\n",
        "\n",
        "# Load NumPy files \n",
        "audio_probabilities = np.load(split_path_audio)\n",
        "text_probabilities = np.load(split_path_text)\n",
        "true_labels = np.load(true_labels_path)\n",
        "\n",
        "if not isinstance(text_probabilities, np.ndarray):\n",
        "    text_probabilities = np.array([t.cpu().numpy() for t in text_probabilities])\n",
        "\n",
        "# Evaluate single models \n",
        "audio_pred_classes = np.argmax(audio_probabilities, axis=1)\n",
        "text_pred_classes = np.argmax(text_probabilities, axis=1)\n",
        "\n",
        "metrics_audio = {\n",
        "    \"accuracy\": accuracy_score(true_labels, audio_pred_classes),\n",
        "    \"f1\": f1_score(true_labels, audio_pred_classes, average='weighted'),\n",
        "    \"precision\": precision_score(true_labels, audio_pred_classes, average='weighted'),\n",
        "    \"recall\": recall_score(true_labels, audio_pred_classes, average='weighted'),\n",
        "    \"kappa\": cohen_kappa_score(true_labels, audio_pred_classes)\n",
        "}\n",
        "\n",
        "metrics_text = {\n",
        "    \"accuracy\": accuracy_score(true_labels, text_pred_classes),\n",
        "    \"f1\": f1_score(true_labels, text_pred_classes, average='weighted'),\n",
        "    \"precision\": precision_score(true_labels, text_pred_classes, average='weighted'),\n",
        "    \"recall\": recall_score(true_labels, text_pred_classes, average='weighted'),\n",
        "    \"kappa\": cohen_kappa_score(true_labels, text_pred_classes)\n",
        "}\n",
        "\n",
        "print(f\"→ Audio Model  | Acc: {metrics_audio['accuracy']:.4f}, F1: {metrics_audio['f1']:.4f}\")\n",
        "print(f\"→ Text  Model  | Acc: {metrics_text['accuracy']:.4f}, F1: {metrics_text['f1']:.4f}\")\n",
        "\n",
        "results = []\n",
        "\n",
        "# Test all weight combinations \n",
        "for alpha_audio, alpha_text in weight_pairs:\n",
        "    final_pred = alpha_audio * audio_probabilities + alpha_text * text_probabilities\n",
        "    final_pred_classes = np.argmax(final_pred, axis=1)\n",
        "\n",
        "    acc = accuracy_score(true_labels, final_pred_classes)\n",
        "    f1 = f1_score(true_labels, final_pred_classes, average='weighted')\n",
        "    precision = precision_score(true_labels, final_pred_classes, average='weighted')\n",
        "    recall = recall_score(true_labels, final_pred_classes, average='weighted')\n",
        "    kappa = cohen_kappa_score(true_labels, final_pred_classes)\n",
        "\n",
        "    results.append({\n",
        "        'alpha_audio': alpha_audio,\n",
        "        'alpha_text': alpha_text,\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'kappa': kappa\n",
        "    })\n",
        "\n",
        "# Create DataFrame \n",
        "df_results = pd.DataFrame(results)\n",
        "df_results = df_results.sort_values(by='f1', ascending=False).reset_index(drop=True)\n",
        "\n",
        "# Best combination \n",
        "best_result = df_results.iloc[0]\n",
        "best_audio = best_result['alpha_audio']\n",
        "best_text = best_result['alpha_text']\n",
        "\n",
        "# Specific combination (audio 0.45, text 0.55) \n",
        "specific_result = df_results[\n",
        "    (df_results['alpha_audio'] == 0.45) & (df_results['alpha_text'] == 0.55)\n",
        "]\n",
        "if specific_result.empty:\n",
        "    specific_result = pd.DataFrame([{\n",
        "        'alpha_audio': 0.45,\n",
        "        'alpha_text': 0.55,\n",
        "        'accuracy': np.nan,\n",
        "        'f1': np.nan,\n",
        "        'precision': np.nan,\n",
        "        'recall': np.nan,\n",
        "        'kappa': np.nan\n",
        "    }])\n",
        "\n",
        "# Save results\n",
        "save_path = os.path.join(base_save_path)\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "# All results\n",
        "df_results.to_csv(os.path.join(save_path, 'all_results.csv'), index=False)\n",
        "\n",
        "# Only best\n",
        "best_result.to_frame().T.to_csv(os.path.join(save_path, 'best_result.csv'), index=False)\n",
        "\n",
        "# Only specific combination (audio 0.45, text 0.55)\n",
        "specific_result.to_csv(os.path.join(save_path, 'specific_result_0.45_0.55.csv'), index=False)\n",
        "\n",
        "print(f\"\\nSaved in: {save_path}\")\n",
        "print(f\"  → Best: Audio {best_audio:.2f}, Text {best_text:.2f} | F1: {best_result['f1']:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
